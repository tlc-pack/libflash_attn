#include "flash.h"
#include "hopper/flash.h"
#include "fa2/flash.h"

#if (__CUDACC_VER_MAJOR__ >= 12)
#  define CUTE_COPY_ATOM_TMA_SM90_ENABLED
#endif

namespace flash_attn{
#if defined(CUTE_COPY_ATOM_TMA_SM90_ENABLED)
void flash_attention_forward(
            half* q_ptr,
            half* k_ptr,
            half* v_ptr,
            half* output_ptr,
            int batch_size,
            int seqlen_q,
            int seqlen_k,
            int num_heads,
            int num_heads_k,
            int head_dim,
            int q_batch_stride,
            int k_batch_stride,
            int v_batch_stride,
            int o_batch_stride,
            int q_head_stride,
            int k_head_stride,
            int v_head_stride,
            int o_head_stride,
            int q_row_stride,
            int k_row_stride,
            int v_row_stride,
            int o_row_stride,
            float softmax_scale,
            bool is_causal,
            int window_size_left,
            int window_size_right,
            cudaStream_t stream){

            flash3::flash_attention_forward(
                q_ptr,
                k_ptr,
                v_ptr,
                output_ptr,
                batch_size,
                seqlen_q,
                seqlen_k,
                num_heads,
                num_heads_k,
                head_dim,
                q_batch_stride,
                k_batch_stride,
                v_batch_stride,
                o_batch_stride,
                q_head_stride,
                k_head_stride,
                v_head_stride,
                o_head_stride,
                q_row_stride,
                k_row_stride,
                v_row_stride,
                o_row_stride,
                softmax_scale,
                is_causal,
                window_size_left,
                window_size_right,
                stream);
            }


void flash_attention_var_len_forward(
            half *q_ptr,
            half *k_ptr,
            half *v_ptr,
            const int *cu_seqlens_q,
            const int *cu_seqlens_k,
            half* output_ptr,
            int batch_size,
            int max_seqlen_q,
            int max_seqlen_k,
            int num_heads,
            int num_heads_k,
            int head_dim,
            int q_head_stride,
            int k_head_stride,
            int v_head_stride,
            int o_head_stride,
            int q_row_stride,
            int k_row_stride,
            int v_row_stride,
            int o_row_stride,
            float softmax_scale,
            bool is_causal,
            int window_size_left,
            int window_size_right,
            cudaStream_t stream){

            flash3::flash_attention_var_len_forward(
                q_ptr,
                k_ptr,
                v_ptr,
                cu_seqlens_q,
                cu_seqlens_k,
                output_ptr,
                batch_size,
                max_seqlen_q,
                max_seqlen_k,
                num_heads,
                num_heads_k,
                head_dim,
                q_head_stride,
                k_head_stride,
                v_head_stride,
                o_head_stride,
                q_row_stride,
                k_row_stride,
                v_row_stride,
                o_row_stride,
                softmax_scale,
                is_causal,
                window_size_left,
                window_size_right,
                stream);
                    }
void flash_attention_splitkv_paged_forward(
            half* q_ptr,
            half* kcache_ptr,
            half* vcache_ptr,
            int32_t* block_table_ptr,
            int32_t* seqlens_k_ptr,
            float* softmax_lse_accum_ptr,
            float* output_accum_ptr,
            half* output_ptr,
            int batch_size,
            int seqlen_q,
            int num_heads,
            int num_heads_k,
            int head_dim,
            int q_batch_stride,
            int k_batch_stride,
            int v_batch_stride,
            int o_batch_stride,
            int q_head_stride,
            int k_head_stride,
            int v_head_stride,
            int o_head_stride,
            int q_row_stride,
            int k_row_stride,
            int v_row_stride,
            int o_row_stride,
            int num_blocks,
            int block_size,
            int max_num_blocks_per_seq,
            int block_table_batch_stride,
            float softmax_scale,
            bool is_causal,
            int window_size_left,
            int window_size_right,
            int num_splits = 0,
            cudaStream_t stream){

            flash2::flash_attention_splitkv_paged_forward(
                q_ptr,
                kcache_ptr,
                vcache_ptr,
                block_table_ptr,
                seqlens_k_ptr,
                softmax_lse_accum_ptr,
                output_accum_ptr,
                output_ptr,
                batch_size,
                seqlen_q,
                num_heads,
                num_heads_k,
                head_dim,
                q_batch_stride,
                k_batch_stride,
                v_batch_stride,
                o_batch_stride,
                q_head_stride,
                k_head_stride,
                v_head_stride,
                o_head_stride,
                q_row_stride,
                k_row_stride,
                v_row_stride,
                o_row_stride,
                num_blocks,
                block_size,
                max_num_blocks_per_seq,
                block_table_batch_stride,
                softmax_scale,
                is_causal,
                window_size_left,
                window_size_right,
                num_splits,
                stream);
            }
#else
void flash_attention_forward(half* q_ptr,
            half* k_ptr,
            half* v_ptr,
            half* output_ptr,
            int batch_size,
            int seqlen_q,
            int seqlen_k,
            int num_heads,
            int num_heads_k,
            int head_dim,
            int q_batch_stride,
            int k_batch_stride,
            int v_batch_stride,
            int o_batch_stride,
            int q_head_stride,
            int k_head_stride,
            int v_head_stride,
            int o_head_stride,
            int q_row_stride,
            int k_row_stride,
            int v_row_stride,
            int o_row_stride,
            float softmax_scale,
            bool is_causal,
            int window_size_left,
            int window_size_right,
            cudaStream_t stream ){

            flash2::flash_attention_forward(
                q_ptr,
                k_ptr,
                v_ptr,
                output_ptr,
                batch_size,
                seqlen_q,
                seqlen_k,
                num_heads,
                num_heads_k,
                head_dim,
                q_batch_stride,
                k_batch_stride,
                v_batch_stride,
                o_batch_stride,
                q_head_stride,
                k_head_stride,
                v_head_stride,
                o_head_stride,
                q_row_stride,
                k_row_stride,
                v_row_stride,
                o_row_stride,
                softmax_scale,
                is_causal,
                window_size_left,
                window_size_right,
                stream);
            }

void flash_attention_var_len_forward(
            half *q_ptr,
            half *k_ptr,
            half *v_ptr,
            const int *cu_seqlens_q,
            const int *cu_seqlens_k,
            half* output_ptr,
            int batch_size,
            int max_seqlen_q,
            int max_seqlen_k,
            int num_heads,
            int num_heads_k,
            int head_dim,
            int q_head_stride,
            int k_head_stride,
            int v_head_stride,
            int o_head_stride,
            int q_row_stride,
            int k_row_stride,
            int v_row_stride,
            int o_row_stride,
            float softmax_scale,
            bool is_causal,
            int window_size_left,
            int window_size_right,
            cudaStream_t stream){

            flash2::flash_attention_var_len_forward(
                q_ptr,
                k_ptr,
                v_ptr,
                cu_seqlens_q,
                cu_seqlens_k,
                output_ptr,
                batch_size,
                max_seqlen_q,
                max_seqlen_k,
                num_heads,
                num_heads_k,
                head_dim,
                q_head_stride,
                k_head_stride,
                v_head_stride,
                o_head_stride,
                q_row_stride,
                k_row_stride,
                v_row_stride,
                o_row_stride,
                softmax_scale,
                is_causal,
                window_size_left,
                window_size_right,
                stream);
                    }

void flash_attention_splitkv_paged_forward(
            half* q_ptr,
            half* kcache_ptr,
            half* vcache_ptr,
            int32_t* block_table_ptr,
            int32_t* seqlens_k_ptr,
            float* softmax_lse_accum_ptr,
            float* output_accum_ptr,
            half* output_ptr,
            int batch_size,
            int seqlen_q,
            int num_heads,
            int num_heads_k,
            int head_dim,
            int q_batch_stride,
            int k_batch_stride,
            int v_batch_stride,
            int o_batch_stride,
            int q_head_stride,
            int k_head_stride,
            int v_head_stride,
            int o_head_stride,
            int q_row_stride,
            int k_row_stride,
            int v_row_stride,
            int o_row_stride,
            int num_blocks,
            int block_size,
            int max_num_blocks_per_seq,
            int block_table_batch_stride,
            float softmax_scale,
            bool is_causal,
            int window_size_left,
            int window_size_right,
            int num_splits,
            cudaStream_t stream){

            flash2::flash_attention_splitkv_paged_forward(
                q_ptr,
                kcache_ptr,
                vcache_ptr,
                block_table_ptr,
                seqlens_k_ptr,
                softmax_lse_accum_ptr,
                output_accum_ptr,
                output_ptr,
                batch_size,
                seqlen_q,
                num_heads,
                num_heads_k,
                head_dim,
                q_batch_stride,
                k_batch_stride,
                v_batch_stride,
                o_batch_stride,
                q_head_stride,
                k_head_stride,
                v_head_stride,
                o_head_stride,
                q_row_stride,
                k_row_stride,
                v_row_stride,
                o_row_stride,
                num_blocks,
                block_size,
                max_num_blocks_per_seq,
                block_table_batch_stride,
                softmax_scale,
                is_causal,
                window_size_left,
                window_size_right,
                num_splits,
                stream);
            }

#endif
}